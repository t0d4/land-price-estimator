{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"./dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "import torchvision.transforms.functional as trF\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image, ImageReadMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's see how the distribution of land price looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGiCAYAAAAFotdwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm5UlEQVR4nO3dfXRU9YH/8U8eJwFJQoJ5WmPI1pYHAVGiMaIslJDYRo9Ylm40SlYj9CGpDTkVjT9MQ1ADsWBAKRStYE9JRXcLi4CQEVaiEkJIZQW0SLdYXG2S3Q1heCiTITO/Pzy568gzmWHIN+/XOR46937nzvd+mwxv7swkQR6PxyMAAADDBAd6AgAAAP5A5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjXXTk1NfX6+6771ZycrKCgoK0du1ar/0ej0fl5eVKSkpSZGSksrKydODAAa8x7e3tys/PV1RUlGJiYlRYWKhjx455jfnwww91xx13KCIiQikpKaqurj5tLm+88YaGDh2qiIgIjRw5Uhs3brzY0wEAAIa66Mg5fvy4brjhBi1ZsuSM+6urq7V48WItW7ZMjY2N6t+/v3JycnTy5ElrTH5+vvbt2ye73a7169ervr5eM2bMsPY7HA5lZ2crNTVVzc3Neu6551RRUaHly5dbY7Zv36777rtPhYWF+uCDDzR58mRNnjxZe/fuvdhTAgAAJvL0gCTPmjVrrNtut9uTmJjoee6556xtHR0dHpvN5vnd737n8Xg8no8++sgjydPU1GSNeeuttzxBQUGezz//3OPxeDy//OUvPQMHDvQ4nU5rzOOPP+4ZMmSIdfv73/++Jzc312s+GRkZnh/84Ac9OSUAAGCIUF8G08GDB9XS0qKsrCxrW3R0tDIyMtTQ0KC8vDw1NDQoJiZG6enp1pisrCwFBwersbFR9957rxoaGjRu3DiFh4dbY3JycjR//nwdPnxYAwcOVENDg0pLS70ePycn57SXz77K6XTK6XRat91ut9rb2xUXF6egoCAfrAAAAPA3j8ejo0ePKjk5WcHBZ39RyqeR09LSIklKSEjw2p6QkGDta2lpUXx8vPckQkMVGxvrNSYtLe20Y3TvGzhwoFpaWs75OGdSVVWlOXPmXMKZAQCAK81nn32ma6655qz7fRo5V7qysjKvqz9HjhzRtddeq4MHD2rAgAEBnFnPuVwu/fu//7smTJigsLCwQE/HWKyz/7HG/sca+x9r7F9Hjx5VWlraef/u9mnkJCYmSpJaW1uVlJRkbW9tbdXo0aOtMW1tbV73O3XqlNrb2637JyYmqrW11WtM9+3zjenefyY2m002m+207bGxsYqKirqQU7xiuVwu9evXT3FxcXxD+RHr7H+ssf+xxv7HGvtX95qe760mPv05OWlpaUpMTNSWLVusbQ6HQ42NjcrMzJQkZWZmqqOjQ83NzdaYrVu3yu12KyMjwxpTX18vl8tljbHb7RoyZIgGDhxojfnq43SP6X4cAADQt1105Bw7dky7d+/W7t27JX35ZuPdu3fr0KFDCgoKUklJiZ5++mmtW7dOe/bs0bRp05ScnKzJkydLkoYNG6Y777xT06dP186dO/X++++ruLhYeXl5Sk5OliTdf//9Cg8PV2Fhofbt26fVq1dr0aJFXi81/fSnP9WmTZu0YMEC/fGPf1RFRYV27dql4uLinq8KAADo9S765apdu3ZpwoQJ1u3u8CgoKNDKlSs1a9YsHT9+XDNmzFBHR4duv/12bdq0SREREdZ9Vq1apeLiYk2cOFHBwcGaMmWKFi9ebO2Pjo5WXV2dioqKNGbMGA0aNEjl5eVeP0vntttuU21trWbPnq0nn3xS3/zmN7V27VqNGDHikhYCAACY5aIjZ/z48fJ4PGfdHxQUpMrKSlVWVp51TGxsrGpra8/5OKNGjdK77757zjFTp07V1KlTzz1hAADQJ/G7qwAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGCki/61DgD6psFPbLgsj2ML8aj6FmlExWY5u4J6dKxP5+X6aFYAeiOu5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjOTzyOnq6tJTTz2ltLQ0RUZG6hvf+Ibmzp0rj8djjfF4PCovL1dSUpIiIyOVlZWlAwcOeB2nvb1d+fn5ioqKUkxMjAoLC3Xs2DGvMR9++KHuuOMORUREKCUlRdXV1b4+HQAA0Ev5PHLmz5+vpUuX6sUXX9THH3+s+fPnq7q6Wi+88II1prq6WosXL9ayZcvU2Nio/v37KycnRydPnrTG5Ofna9++fbLb7Vq/fr3q6+s1Y8YMa7/D4VB2drZSU1PV3Nys5557ThUVFVq+fLmvTwkAAPRCob4+4Pbt23XPPfcoNzdXkjR48GD97ne/086dOyV9eRWnpqZGs2fP1j333CNJ+s1vfqOEhAStXbtWeXl5+vjjj7Vp0yY1NTUpPT1dkvTCCy/ou9/9rn7xi18oOTlZq1atUmdnp1555RWFh4fr+uuv1+7du7Vw4UKvGAIAAH2TzyPntttu0/Lly/XJJ5/oW9/6lv7jP/5D7733nhYuXChJOnjwoFpaWpSVlWXdJzo6WhkZGWpoaFBeXp4aGhoUExNjBY4kZWVlKTg4WI2Njbr33nvV0NCgcePGKTw83BqTk5Oj+fPn6/Dhwxo4cOBpc3M6nXI6ndZth8MhSXK5XHK5XL5eisuqe/69/TyudH15nW0hnvMP8sXjBHu8/uyJvvj/04Xoy1/Hlwtr7F8Xuq4+j5wnnnhCDodDQ4cOVUhIiLq6uvTMM88oPz9fktTS0iJJSkhI8LpfQkKCta+lpUXx8fHeEw0NVWxsrNeYtLS0047Rve9MkVNVVaU5c+actr2urk79+vW7lNO94tjt9kBPoU/oi+tcfcvlfby56e4eH2Pjxo0+mIm5+uLX8eXGGvvHiRMnLmiczyPn9ddf16pVq1RbW2u9hFRSUqLk5GQVFBT4+uEuSllZmUpLS63bDodDKSkpys7OVlRUVABn1nMul0t2u12TJk1SWFhYoKdjrL68ziMqNl+Wx7EFezQ33a2ndgXL6Q7q0bH2VuT4aFZm6ctfx5cLa+xf3a/EnI/PI+exxx7TE088oby8PEnSyJEj9Ze//EVVVVUqKChQYmKiJKm1tVVJSUnW/VpbWzV69GhJUmJiotra2ryOe+rUKbW3t1v3T0xMVGtrq9eY7tvdY77OZrPJZrOdtj0sLMyYL0KTzuVK1hfX2dnVs+C46MdzB/X4Mfva/0cXqy9+HV9urLF/XOia+vzTVSdOnFBwsPdhQ0JC5HZ/eek5LS1NiYmJ2rJli7Xf4XCosbFRmZmZkqTMzEx1dHSoubnZGrN161a53W5lZGRYY+rr671el7Pb7RoyZMgZX6oCAAB9i88j5+6779YzzzyjDRs26NNPP9WaNWu0cOFC3XvvvZKkoKAglZSU6Omnn9a6deu0Z88eTZs2TcnJyZo8ebIkadiwYbrzzjs1ffp07dy5U++//76Ki4uVl5en5ORkSdL999+v8PBwFRYWat++fVq9erUWLVrk9XIUAADou3z+ctULL7ygp556Sj/+8Y/V1tam5ORk/eAHP1B5ebk1ZtasWTp+/LhmzJihjo4O3X777dq0aZMiIiKsMatWrVJxcbEmTpyo4OBgTZkyRYsXL7b2R0dHq66uTkVFRRozZowGDRqk8vJyPj4OAAAk+SFyBgwYoJqaGtXU1Jx1TFBQkCorK1VZWXnWMbGxsaqtrT3nY40aNUrvvvvupU4VAAAYjN9dBQAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIwUGugJAH3N4Cc2BHoKANAncCUHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABjJL5Hz+eef64EHHlBcXJwiIyM1cuRI7dq1y9rv8XhUXl6upKQkRUZGKisrSwcOHPA6Rnt7u/Lz8xUVFaWYmBgVFhbq2LFjXmM+/PBD3XHHHYqIiFBKSoqqq6v9cToAAKAX8nnkHD58WGPHjlVYWJjeeustffTRR1qwYIEGDhxojamurtbixYu1bNkyNTY2qn///srJydHJkyetMfn5+dq3b5/sdrvWr1+v+vp6zZgxw9rvcDiUnZ2t1NRUNTc367nnnlNFRYWWL1/u61MCAAC9UKivDzh//nylpKRoxYoV1ra0tDTrf3s8HtXU1Gj27Nm65557JEm/+c1vlJCQoLVr1yovL08ff/yxNm3apKamJqWnp0uSXnjhBX33u9/VL37xCyUnJ2vVqlXq7OzUK6+8ovDwcF1//fXavXu3Fi5c6BVDAACgb/J55Kxbt045OTmaOnWqtm3bpr/7u7/Tj3/8Y02fPl2SdPDgQbW0tCgrK8u6T3R0tDIyMtTQ0KC8vDw1NDQoJibGChxJysrKUnBwsBobG3XvvfeqoaFB48aNU3h4uDUmJydH8+fP1+HDh72uHHVzOp1yOp3WbYfDIUlyuVxyuVy+XorLqnv+vf08rnS+WGdbiMdX0zGSLdjj9WdP8P1wZjxf+B9r7F8Xuq4+j5w///nPWrp0qUpLS/Xkk0+qqalJjz76qMLDw1VQUKCWlhZJUkJCgtf9EhISrH0tLS2Kj4/3nmhoqGJjY73GfPUK0VeP2dLScsbIqaqq0pw5c07bXldXp379+l3iGV9Z7HZ7oKfQJ/Rknatv8eFEDDY33d3jY2zcuNEHMzEXzxf+xxr7x4kTJy5onM8jx+12Kz09Xc8++6wk6cYbb9TevXu1bNkyFRQU+PrhLkpZWZlKS0ut2w6HQykpKcrOzlZUVFQAZ9ZzLpdLdrtdkyZNUlhYWKCnYyxfrPOIis0+npVZbMEezU1366ldwXK6g3p0rL0VOT6alVl4vvA/1ti/ul+JOR+fR05SUpKGDx/utW3YsGH613/9V0lSYmKiJKm1tVVJSUnWmNbWVo0ePdoa09bW5nWMU6dOqb293bp/YmKiWltbvcZ03+4e83U2m002m+207WFhYcZ8EZp0Lleynqyzs6tnf3H3FU53UI/Xiu+Fc+P5wv9YY/+40DX1+aerxo4dq/3793tt++STT5SamirpyzchJyYmasuWLdZ+h8OhxsZGZWZmSpIyMzPV0dGh5uZma8zWrVvldruVkZFhjamvr/d6Xc5ut2vIkCFnfKkKAAD0LT6PnJkzZ2rHjh169tln9ac//Um1tbVavny5ioqKJElBQUEqKSnR008/rXXr1mnPnj2aNm2akpOTNXnyZElfXvm58847NX36dO3cuVPvv/++iouLlZeXp+TkZEnS/fffr/DwcBUWFmrfvn1avXq1Fi1a5PVyFAAA6Lt8/nLVzTffrDVr1qisrEyVlZVKS0tTTU2N8vPzrTGzZs3S8ePHNWPGDHV0dOj222/Xpk2bFBERYY1ZtWqViouLNXHiRAUHB2vKlClavHixtT86Olp1dXUqKirSmDFjNGjQIJWXl/PxcQAAIMkPkSNJd911l+66666z7g8KClJlZaUqKyvPOiY2Nla1tbXnfJxRo0bp3XffveR5AgAAc/G7qwAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkUIDPQEA8JfBT2wI9BQu2qfzcgM9BcAYXMkBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGMnvkTNv3jwFBQWppKTE2nby5EkVFRUpLi5OV111laZMmaLW1lav+x06dEi5ubnq16+f4uPj9dhjj+nUqVNeY9555x3ddNNNstlsuu6667Ry5Up/nw4AAOgl/Bo5TU1N+tWvfqVRo0Z5bZ85c6befPNNvfHGG9q2bZu++OILfe9737P2d3V1KTc3V52dndq+fbteffVVrVy5UuXl5daYgwcPKjc3VxMmTNDu3btVUlKiRx55RJs3b/bnKQEAgF7Cb5Fz7Ngx5efn66WXXtLAgQOt7UeOHNGvf/1rLVy4UN/+9rc1ZswYrVixQtu3b9eOHTskSXV1dfroo4/029/+VqNHj9Z3vvMdzZ07V0uWLFFnZ6ckadmyZUpLS9OCBQs0bNgwFRcX6x//8R/1/PPP++uUAABAL+K3yCkqKlJubq6ysrK8tjc3N8vlcnltHzp0qK699lo1NDRIkhoaGjRy5EglJCRYY3JycuRwOLRv3z5rzNePnZOTYx0DAAD0baH+OOhrr72mP/zhD2pqajptX0tLi8LDwxUTE+O1PSEhQS0tLdaYrwZO9/7ufeca43A49Le//U2RkZGnPbbT6ZTT6bRuOxwOSZLL5ZLL5brIs7yydM+/t5/Hlc4X62wL8fhqOkayBXu8/uxrLsf3MM8X/sca+9eFrqvPI+ezzz7TT3/6U9ntdkVERPj68D1SVVWlOXPmnLa9rq5O/fr1C8CMfM9utwd6Cn1CT9a5+hYfTsRgc9PdgZ5CQGzcuPGyPRbPF/7HGvvHiRMnLmiczyOnublZbW1tuummm6xtXV1dqq+v14svvqjNmzers7NTHR0dXldzWltblZiYKElKTEzUzp07vY7b/emrr475+ieyWltbFRUVdcarOJJUVlam0tJS67bD4VBKSoqys7MVFRV16Sd9BXC5XLLb7Zo0aZLCwsICPR1j+WKdR1Tw5vhzsQV7NDfdrad2BcvpDgr0dC67vRU5fn8Mni/8jzX2r+5XYs7H55EzceJE7dmzx2vbQw89pKFDh+rxxx9XSkqKwsLCtGXLFk2ZMkWStH//fh06dEiZmZmSpMzMTD3zzDNqa2tTfHy8pC9rOCoqSsOHD7fGfP1fPHa73TrGmdhsNtlsttO2h4WFGfNFaNK5XMl6ss7Orr73F/elcLqD+uRaXc7vX54v/I819o8LXVOfR86AAQM0YsQIr239+/dXXFyctb2wsFClpaWKjY1VVFSUfvKTnygzM1O33nqrJCk7O1vDhw/Xgw8+qOrqarW0tGj27NkqKiqyIuWHP/yhXnzxRc2aNUsPP/ywtm7dqtdff10bNmzw9SkBAIBeyC9vPD6f559/XsHBwZoyZYqcTqdycnL0y1/+0tofEhKi9evX60c/+pEyMzPVv39/FRQUqLKy0hqTlpamDRs2aObMmVq0aJGuueYavfzyy8rJ8f+lXgAAcOW7LJHzzjvveN2OiIjQkiVLtGTJkrPeJzU19bxvwBs/frw++OADX0wRAAAYht9dBQAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASD6PnKqqKt18880aMGCA4uPjNXnyZO3fv99rzMmTJ1VUVKS4uDhdddVVmjJlilpbW73GHDp0SLm5uerXr5/i4+P12GOP6dSpU15j3nnnHd10002y2Wy67rrrtHLlSl+fDgAA6KV8Hjnbtm1TUVGRduzYIbvdLpfLpezsbB0/ftwaM3PmTL355pt64403tG3bNn3xxRf63ve+Z+3v6upSbm6uOjs7tX37dr366qtauXKlysvLrTEHDx5Ubm6uJkyYoN27d6ukpESPPPKINm/e7OtTAgAAvVCorw+4adMmr9srV65UfHy8mpubNW7cOB05ckS//vWvVVtbq29/+9uSpBUrVmjYsGHasWOHbr31VtXV1emjjz7S22+/rYSEBI0ePVpz587V448/roqKCoWHh2vZsmVKS0vTggULJEnDhg3Te++9p+eff145OTm+Pi0AANDL+Dxyvu7IkSOSpNjYWElSc3OzXC6XsrKyrDFDhw7Vtddeq4aGBt16661qaGjQyJEjlZCQYI3JycnRj370I+3bt0833nijGhoavI7RPaakpOSsc3E6nXI6ndZth8MhSXK5XHK5XD0+10Dqnn9vP48rnS/W2Rbi8dV0jGQL9nj92ddcju9hni/8jzX2rwtdV79GjtvtVklJicaOHasRI0ZIklpaWhQeHq6YmBivsQkJCWppabHGfDVwuvd37zvXGIfDob/97W+KjIw8bT5VVVWaM2fOadvr6urUr1+/SzvJK4zdbg/0FPqEnqxz9S0+nIjB5qa7Az2FgNi4ceNleyyeL/yPNfaPEydOXNA4v0ZOUVGR9u7dq/fee8+fD3PBysrKVFpaat12OBxKSUlRdna2oqKiAjiznnO5XLLb7Zo0aZLCwsICPR1j+WKdR1TwvrFzsQV7NDfdrad2BcvpDgr0dC67vRX+f7md5wv/Y439q/uVmPPxW+QUFxdr/fr1qq+v1zXXXGNtT0xMVGdnpzo6Oryu5rS2tioxMdEas3PnTq/jdX/66qtjvv6JrNbWVkVFRZ3xKo4k2Ww22Wy207aHhYUZ80Vo0rlcyXqyzs6uvvcX96VwuoP65Fpdzu9fni/8jzX2jwtdU59/usrj8ai4uFhr1qzR1q1blZaW5rV/zJgxCgsL05YtW6xt+/fv16FDh5SZmSlJyszM1J49e9TW1maNsdvtioqK0vDhw60xXz1G95juYwAAgL7N51dyioqKVFtbq3/7t3/TgAEDrPfQREdHKzIyUtHR0SosLFRpaaliY2MVFRWln/zkJ8rMzNStt94qScrOztbw4cP14IMPqrq6Wi0tLZo9e7aKioqsKzE//OEP9eKLL2rWrFl6+OGHtXXrVr3++uvasGGDr08JAAD0Qj6/krN06VIdOXJE48ePV1JSkvXf6tWrrTHPP/+87rrrLk2ZMkXjxo1TYmKifv/731v7Q0JCtH79eoWEhCgzM1MPPPCApk2bpsrKSmtMWlqaNmzYILvdrhtuuEELFizQyy+/zMfHAQCAJD9cyfF4zv+xz4iICC1ZskRLliw565jU1NTzfspg/Pjx+uCDDy56jgAAwHz87ioAAGAkIgcAABjJ7z/xGPCnwU9c3jea20I8qr7ly5910xc/3gwAvQlXcgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEbiF3QCwBXkcvzSWV//otlP5+X6YFaA73ElBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARgoN9AQAAL3b4Cc2BHoKF+3TebmBngIuA67kAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASPycHlt74sy4AADibXn8lZ8mSJRo8eLAiIiKUkZGhnTt3BnpKAADgCtCrr+SsXr1apaWlWrZsmTIyMlRTU6OcnBzt379f8fHxgZ4eAOAK5e8r17YQj6pvkUZUbJazK8gnx+SnNF+8Xn0lZ+HChZo+fboeeughDR8+XMuWLVO/fv30yiuvBHpqAAAgwHrtlZzOzk41NzerrKzM2hYcHKysrCw1NDSc8T5Op1NOp9O6feTIEUlSe3u7XC6XT+eXUbXFp8c7H1uwR7NvdGv0//u9nO5L+1dDr/1iuIxC3R6dOOFWqCtYXZe4zjg31tj/WGP/88caX/ez131ynMupsWyiX4579OhRSZLH4znnuF7799r//M//qKurSwkJCV7bExIS9Mc//vGM96mqqtKcOXNO256WluaXOV5u9wd6An0E6+x/rLH/scb+xxpLgxb49/hHjx5VdHT0Wff32si5FGVlZSotLbVuu91utbe3Ky4uTkFBvftfMw6HQykpKfrss88UFRUV6OkYi3X2P9bY/1hj/2ON/cvj8ejo0aNKTk4+57heGzmDBg1SSEiIWltbvba3trYqMTHxjPex2Wyy2Wxe22JiYvw1xYCIioriG+oyYJ39jzX2P9bY/1hj/znXFZxuvfaNx+Hh4RozZoy2bPm/97643W5t2bJFmZmZAZwZAAC4EvTaKzmSVFpaqoKCAqWnp+uWW25RTU2Njh8/roceeijQUwMAAAHWqyPnn/7pn/Tf//3fKi8vV0tLi0aPHq1Nmzad9mbkvsBms+nnP//5aS/HwbdYZ/9jjf2PNfY/1vjKEOQ53+evAAAAeqFe+54cAACAcyFyAACAkYgcAABgJCIHAAAYicjp5ZYuXapRo0ZZP3AqMzNTb731VqCnZbR58+YpKChIJSUlgZ6KMSoqKhQUFOT139ChQwM9LeN8/vnneuCBBxQXF6fIyEiNHDlSu3btCvS0jDJ48ODTvpaDgoJUVFQU6Kn1Sb36I+SQrrnmGs2bN0/f/OY35fF49Oqrr+qee+7RBx98oOuvvz7Q0zNOU1OTfvWrX2nUqFGBnopxrr/+er399tvW7dBQnp586fDhwxo7dqwmTJigt956S1dffbUOHDiggQMHBnpqRmlqalJXV5d1e+/evZo0aZKmTp0awFn1XTyL9HJ333231+1nnnlGS5cu1Y4dO4gcHzt27Jjy8/P10ksv6emnnw70dIwTGhp61l/Jgp6bP3++UlJStGLFCmubKb+c+Epy9dVXe92eN2+evvGNb+gf/uEfAjSjvo2XqwzS1dWl1157TcePH+dXW/hBUVGRcnNzlZWVFeipGOnAgQNKTk7W3//93ys/P1+HDh0K9JSMsm7dOqWnp2vq1KmKj4/XjTfeqJdeeinQ0zJaZ2enfvvb3+rhhx/u9b8EurfiSo4B9uzZo8zMTJ08eVJXXXWV1qxZo+HDhwd6WkZ57bXX9Ic//EFNTU2BnoqRMjIytHLlSg0ZMkR//etfNWfOHN1xxx3au3evBgwYEOjpGeHPf/6zli5dqtLSUj355JNqamrSo48+qvDwcBUUFAR6ekZau3atOjo69M///M+BnkqfxU88NkBnZ6cOHTqkI0eO6F/+5V/08ssva9u2bYSOj3z22WdKT0+X3W633oszfvx4jR49WjU1NYGdnKE6OjqUmpqqhQsXqrCwMNDTMUJ4eLjS09O1fft2a9ujjz6qpqYmNTQ0BHBm5srJyVF4eLjefPPNQE+lz+LlKgOEh4fruuuu05gxY1RVVaUbbrhBixYtCvS0jNHc3Ky2tjbddNNNCg0NVWhoqLZt26bFixcrNDTU602G8I2YmBh961vf0p/+9KdAT8UYSUlJp/3DZ9iwYbws6Cd/+ctf9Pbbb+uRRx4J9FT6NF6uMpDb7ZbT6Qz0NIwxceJE7dmzx2vbQw89pKFDh+rxxx9XSEhIgGZmrmPHjuk///M/9eCDDwZ6KsYYO3as9u/f77Xtk08+UWpqaoBmZLYVK1YoPj5eubm5gZ5Kn0bk9HJlZWX6zne+o2uvvVZHjx5VbW2t3nnnHW3evDnQUzPGgAEDNGLECK9t/fv3V1xc3GnbcWl+9rOf6e6771Zqaqq++OIL/fznP1dISIjuu+++QE/NGDNnztRtt92mZ599Vt///ve1c+dOLV++XMuXLw/01Izjdru1YsUKFRQU8KMQAozV7+Xa2to0bdo0/fWvf1V0dLRGjRqlzZs3a9KkSYGeGnDB/uu//kv33Xef/vd//1dXX321br/9du3YseO0j+Pi0t18881as2aNysrKVFlZqbS0NNXU1Cg/Pz/QUzPO22+/rUOHDunhhx8O9FT6PN54DAAAjMQbjwEAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEb6/zD0iyHIq2iTAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_all = pd.read_csv(\n",
    "    os.path.join(DATASET_ROOT, \"data_catalogue.csv\"),\n",
    "    index_col=False\n",
    ")\n",
    "# df_all[\"landprice\"].hist(bins=10)\n",
    "df_all[\"log_landprice\"] = np.log10(df_all[\"landprice\"])\n",
    "df_all[\"log_landprice\"].hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Only once] Split into train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATE = 0.8\n",
    "VAL_RATE = 0.1\n",
    "TEST_RATE = 0.1\n",
    "assert( TRAIN_RATE + VAL_RATE + TEST_RATE == 1.0 )\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(DATASET_ROOT, \"data_catalogue.csv\"),\n",
    "    index_col=False\n",
    ")\n",
    "data_count = len(df)\n",
    "\n",
    "shuffled_indices = list(range(data_count))\n",
    "random.shuffle(shuffled_indices)\n",
    "\n",
    "train_df = df.iloc[\n",
    "    shuffled_indices[:int(data_count*TRAIN_RATE)], :\n",
    "]\n",
    "val_df = df.iloc[\n",
    "    shuffled_indices[int(data_count*TRAIN_RATE):int(data_count*(TRAIN_RATE+VAL_RATE))] , :\n",
    "]\n",
    "test_df = df.iloc[\n",
    "    shuffled_indices[int(data_count*(TRAIN_RATE+VAL_RATE)):], :\n",
    "]\n",
    "\n",
    "train_df.to_csv(\n",
    "    os.path.join(DATASET_ROOT,\"train_data_catalogue.csv\"),\n",
    "    index=False\n",
    ")\n",
    "val_df.to_csv(\n",
    "    os.path.join(DATASET_ROOT,\"val_data_catalogue.csv\"),\n",
    "    index=False\n",
    ")\n",
    "test_df.to_csv(\n",
    "    os.path.join(DATASET_ROOT,\"test_data_catalogue.csv\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandPriceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_root_dir_path: str,\n",
    "        catalogue_csv_name: str,\n",
    "        target_img_size: Union[Tuple[int, int], None] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(dataset_root_dir_path, catalogue_csv_name),\n",
    "            index_col=False\n",
    "        )\n",
    "        self.img_paths = [\n",
    "            os.path.join(dataset_root_dir_path, \"images\", img_filename)\n",
    "            for img_filename in df[\"img_filename\"].to_list()\n",
    "        ]\n",
    "        self.segmap_paths = [\n",
    "            os.path.join(dataset_root_dir_path, \"masked_images\", mask_filename)\n",
    "            for mask_filename in df[\"mask_filename\"].to_list()\n",
    "        ]\n",
    "        self.landprices = df[\"landprice\"].to_list()\n",
    "        self.target_img_size = target_img_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[str, Tensor, Tensor, Tensor]:\n",
    "        img_path = self.img_paths[index]\n",
    "        image = read_image(\n",
    "            path=img_path,\n",
    "            mode=ImageReadMode.RGB\n",
    "        )\n",
    "        # extract 000001 from satellite000001.png\n",
    "        image_id = os.path.basename(img_path)[9:15]\n",
    "\n",
    "        segmap_path = self.segmap_paths[index]\n",
    "        segmap = read_image(\n",
    "            path=segmap_path,\n",
    "            mode=ImageReadMode.GRAY\n",
    "        )\n",
    "\n",
    "        landprice = self.landprices[index]\n",
    "        landprice = torch.tensor(landprice)\n",
    "\n",
    "        if self.target_img_size:\n",
    "            image = trF.resize(\n",
    "                img=image,\n",
    "                size=self.target_img_size,\n",
    "            )\n",
    "            segmap = nnF.interpolate(\n",
    "                input=segmap.unsqueeze(0),\n",
    "                size=self.target_img_size,\n",
    "                mode=\"nearest\",\n",
    "            )\n",
    "\n",
    "        # one-hot encoding for segmap\n",
    "        SEGMAP_CLASS_NUM = 8\n",
    "        segmap = nnF.one_hot(segmap.long(), SEGMAP_CLASS_NUM).transpose(1, 4).squeeze()\n",
    "        # segmap is like torch.Size([SEGMAP_CLASS_NUM, SEGMAP_H, SEGMAP_W])\n",
    "\n",
    "        return image_id, image.float(), segmap.float(), landprice.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_IMAGE_SIZE = 480\n",
    "\n",
    "train_ds = LandPriceDataset(\n",
    "    dataset_root_dir_path=DATASET_ROOT,\n",
    "    catalogue_csv_name=\"train_data_catalogue.csv\",\n",
    "    target_img_size=(TARGET_IMAGE_SIZE, TARGET_IMAGE_SIZE)\n",
    ")\n",
    "val_ds = LandPriceDataset(\n",
    "    dataset_root_dir_path=DATASET_ROOT,\n",
    "    catalogue_csv_name=\"val_data_catalogue.csv\",\n",
    "    target_img_size=(TARGET_IMAGE_SIZE, TARGET_IMAGE_SIZE),\n",
    ")\n",
    "test_ds = LandPriceDataset(\n",
    "    dataset_root_dir_path=DATASET_ROOT,\n",
    "    catalogue_csv_name=\"test_data_catalogue.csv\",\n",
    "    target_img_size=(TARGET_IMAGE_SIZE, TARGET_IMAGE_SIZE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    "\n",
    "class FeatureMapCompresser(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # (height_of_feature_map)x(width_of_feature_map)x(num_filter_for_each_channel)\n",
    "        self.feat_wise_fc_to_128 = nn.Linear(in_features=29*29*2, out_features=128)\n",
    "        self.feat_wise_fc_last = nn.Linear(in_features=128, out_features=32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.feat_wise_fc_to_128(x)\n",
    "        x = self.feat_wise_fc_last(x)\n",
    "        return x\n",
    "\n",
    "class EfficientNetBasedFeatureExtractor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        whole_effnet = efficientnet_v2_m(\n",
    "            weights=EfficientNet_V2_M_Weights.IMAGENET1K_V1\n",
    "        )\n",
    "        # omit the last classification layer\n",
    "        self.effnet_without_classifier = nn.Sequential(\n",
    "            whole_effnet.features,\n",
    "            whole_effnet.avgpool,\n",
    "        )\n",
    "        self.fc_to_512 = nn.Linear(in_features=1280, out_features=512)\n",
    "        self.fc_to_256 = nn.Linear(in_features=512, out_features=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.effnet_without_classifier(x)  # size of x == torch.Size([BATCH, 1280, 1, 1])\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.fc_to_512(x)\n",
    "        x = self.fc_to_256(x)\n",
    "        return x\n",
    "\n",
    "class LandNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.effnet_feat_extractor = EfficientNetBasedFeatureExtractor()\n",
    "        self.conv_feat_extractor = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=6, # number of meaningful class in segmentation maps\n",
    "                out_channels=12,\n",
    "                kernel_size=4,\n",
    "                groups=6  # MUST be the same as `in_channels`. This works as depth-wise convolution\n",
    "                # `out_channels`/`in_channels` (= 2) dedicated filters are applied to each channel in the input\n",
    "            ),\n",
    "            nn.MaxPool2d(kernel_size=16),  # each side of feature map becomes 1/16 the length of previous one\n",
    "            # # this another convolution seems to be unnecessary\n",
    "            # nn.Conv2d(\n",
    "            #     in_channels=18,\n",
    "            #     out_channels=36,\n",
    "            #     kernel_size=2,\n",
    "            #     groups=3,\n",
    "            # ),\n",
    "            # nn.MaxPool2d(kernel_size=4),\n",
    "        )\n",
    "        self.feat_compresser_for_build = FeatureMapCompresser()\n",
    "        self.feat_compresser_for_road = FeatureMapCompresser()\n",
    "        self.feat_compresser_for_water = FeatureMapCompresser()\n",
    "        self.feat_compresser_for_barren = FeatureMapCompresser()\n",
    "        self.feat_compresser_for_forest = FeatureMapCompresser()\n",
    "        self.feat_compresser_for_agri = FeatureMapCompresser()\n",
    "\n",
    "        self.trailing_fc_to_256 = nn.Linear(in_features=256+32*6, out_features=256)\n",
    "        self.trailing_fc_to_64 = nn.Linear(in_features=256, out_features=64)\n",
    "        self.last_fc = nn.Linear(in_features=64, out_features=1)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, image, segmap):\n",
    "        feat_image = self.effnet_feat_extractor(image)\n",
    "\n",
    "        y = self.conv_feat_extractor(segmap)\n",
    "        feat_build = self.feat_compresser_for_build(y[:, :2, ...])\n",
    "        feat_road = self.feat_compresser_for_road(y[:, 2:4, ...])\n",
    "        feat_water = self.feat_compresser_for_water(y[:, 4:6, ...])\n",
    "        feat_barren = self.feat_compresser_for_barren(y[:, 6:8, ...])\n",
    "        feat_forest = self.feat_compresser_for_forest(y[:, 8:10, ...])\n",
    "        feat_agri = self.feat_compresser_for_agri(y[:, 10:12, ...])\n",
    "\n",
    "        feat_concat = torch.cat(\n",
    "            tensors=[feat_image, feat_build, feat_road, feat_water, feat_barren, feat_forest, feat_agri],\n",
    "            dim=1\n",
    "        )\n",
    "        overall_feat = self.trailing_fc_to_256(feat_concat)\n",
    "        overall_feat = self.dropout(overall_feat)\n",
    "        overall_feat = self.trailing_fc_to_64(overall_feat)\n",
    "        pred_value = self.last_fc(overall_feat)\n",
    "\n",
    "        return pred_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  55094529\n"
     ]
    }
   ],
   "source": [
    "landnet = LandNet().to(device)\n",
    "params = 0\n",
    "for p in landnet.parameters():\n",
    "    if p.requires_grad:\n",
    "        params += p.numel()\n",
    "print(\"number of parameters: \", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the shape is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_image = torch.randn(2, 3, 480, 480)\n",
    "# dummy_segmap = torch.randn(2, 6, 480, 480)\n",
    "# landnet(dummy_image, dummy_segmap)\n",
    "\n",
    "# # features only: torch.Size([2, 1280, 15, 15])\n",
    "# # features+avgpool: torch.Size([2, 1280, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Based on the observation at the beginning of this notebook,\n",
    "# let's employ Rooted Mean Squared Logarithmic Error\n",
    "# https://atmarkit.itmedia.co.jp/ait/articles/2106/02/news021.html\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.constant = 10\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(pred + self.constant), torch.log(actual + self.constant)))\n",
    "\n",
    "loss_func = RMSLELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=landnet.parameters(),\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "def train(\n",
    "    dataloader: DataLoader,\n",
    "    model: LandNet,\n",
    "    loss_func: RMSLELoss,\n",
    "    optimizer: torch.optim.AdamW,\n",
    "    current_epoch: int,\n",
    "    writer: SummaryWriter,\n",
    "):\n",
    "    whole_size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    mean_loss_in_epoch = .0\n",
    "    mean_deviation_in_epoch = .0\n",
    "    for batch_pos, (image_id, image, segmap, landprice) in enumerate(dataloader):\n",
    "\n",
    "        image, landprice = image.to(device), landprice.to(device)\n",
    "        # omit segmaps for Ignore (0), Background (1)\n",
    "        segmap = segmap[:, 2:, ...]\n",
    "        segmap = segmap.to(device)\n",
    "\n",
    "        # compute the prediction\n",
    "        pred = model(image, segmap)\n",
    "        loss = loss_func(pred, landprice)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # update mean loss\n",
    "        loss = loss.item()\n",
    "        mean_loss_in_epoch = (batch_pos*mean_loss_in_epoch + loss)/(batch_pos+1)\n",
    "        # update mean deviation rate\n",
    "        deviations = torch.abs(pred - landprice) / landprice\n",
    "        mean_deviation_in_batch = deviations.mean().item()\n",
    "        mean_deviation_in_epoch = (batch_pos*mean_deviation_in_epoch + mean_deviation_in_batch)/(batch_pos+1)\n",
    "\n",
    "        if batch_pos % 100 == 0:\n",
    "            current_pos = batch_pos*len(image)\n",
    "            print(dedent(\n",
    "                f\"\"\"[{current_pos:>5d}/{whole_size:>5d}]\\n\\\n",
    "                loss: {loss:>7f}\\n\\\n",
    "                current_mean_loss_in_epoch: {mean_loss_in_epoch:>7f}\\n\\\n",
    "                current_mean_deviation_rate_in_epoch: {mean_deviation_in_epoch:>7f}\"\"\"))\n",
    "\n",
    "    writer.add_scalar(\n",
    "        tag=\"loss/train\",\n",
    "        scalar_value=mean_loss_in_epoch,\n",
    "        global_step=current_epoch,\n",
    "    )\n",
    "    writer.add_scalar(\n",
    "        tag=\"deviation_rate/train\",\n",
    "        scalar_value=mean_deviation_in_epoch,\n",
    "        global_step=current_epoch,\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "def validate(\n",
    "    dataloader: DataLoader,\n",
    "    model: LandNet,\n",
    "    loss_func: RMSLELoss,\n",
    "    current_epoch: int,\n",
    "    writer: Union[SummaryWriter, None] = None,\n",
    "    summary_csv_path: Union[str, None] = None,\n",
    "):\n",
    "    whole_size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    mean_loss_in_epoch = .0\n",
    "    mean_deviation_in_epoch = .0\n",
    "    image_id_log = []\n",
    "    deviation_log = []\n",
    "    pred_log = []\n",
    "    landprice_log = []\n",
    "    with torch.no_grad():\n",
    "        for batch_pos, (image_id, image, segmap, landprice) in enumerate(dataloader):\n",
    "\n",
    "            image, landprice = image.to(device), landprice.to(device)\n",
    "            segmap = segmap[:, 2:, ...]\n",
    "            segmap = segmap.to(device)\n",
    "\n",
    "            # compute the prediction\n",
    "            pred = model(image, segmap)\n",
    "            loss = loss_func(pred, landprice)\n",
    "\n",
    "            # update mean loss\n",
    "            loss = loss.item()\n",
    "            mean_loss_in_epoch = (batch_pos*mean_loss_in_epoch + loss)/(batch_pos+1)\n",
    "            # update mean deviation rate\n",
    "            deviations = torch.abs(pred.squeeze() - landprice) / landprice\n",
    "            mean_deviation_in_batch = deviations.mean().item()\n",
    "            mean_deviation_in_epoch = (batch_pos*mean_deviation_in_epoch + mean_deviation_in_batch)/(batch_pos+1)\n",
    "\n",
    "            if summary_csv_path:\n",
    "                image_id_log += image_id\n",
    "                pred_log += pred.squeeze().cpu().tolist()\n",
    "                landprice_log += landprice.cpu().tolist()\n",
    "                deviation_log += deviations.cpu().tolist()\n",
    "\n",
    "            if batch_pos % 100 == 0:\n",
    "                current_pos = batch_pos*len(image)\n",
    "                print(dedent(\n",
    "                    f\"[{current_pos:>5d}/{whole_size:>5d}]\\n\\\n",
    "                    loss: {loss:>7f}\\n\\\n",
    "                    current_mean_loss_in_epoch: {mean_loss_in_epoch:>7f}\\n\\\n",
    "                    current_mean_deviation_rate_in_epoch: {mean_deviation_in_epoch:>7f}\"))\n",
    "\n",
    "    if summary_csv_path:\n",
    "        df = pd.DataFrame(\n",
    "            data={\n",
    "                \"image_id\": image_id_log,\n",
    "                \"prediction\": pred_log,\n",
    "                \"true_landprice\": landprice_log,\n",
    "                \"deviation\": deviation_log,\n",
    "            }\n",
    "        )\n",
    "        df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar(\n",
    "            tag=\"loss/val\",\n",
    "            scalar_value=mean_loss_in_epoch,\n",
    "            global_step=current_epoch,\n",
    "        )\n",
    "        writer.add_scalar(\n",
    "            tag=\"deviation_rate/val\",\n",
    "            scalar_value=mean_deviation_in_epoch,\n",
    "            global_step=current_epoch,\n",
    "        )\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size=BATCH_SIZE)\n",
    "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10500/20794]\n",
      "                loss: 0.534398\n",
      "                current_mean_loss_in_epoch: 1.167936\n",
      "                current_mean_deviation_rate_in_epoch: 1.519949\n",
      "[11000/20794]\n",
      "                loss: 1.884604\n",
      "                current_mean_loss_in_epoch: 1.171060\n",
      "                current_mean_deviation_rate_in_epoch: 1.527004\n",
      "[11500/20794]\n",
      "                loss: 1.934586\n",
      "                current_mean_loss_in_epoch: 1.174081\n",
      "                current_mean_deviation_rate_in_epoch: 1.521944\n",
      "[12000/20794]\n",
      "                loss: 1.090881\n",
      "                current_mean_loss_in_epoch: 1.175180\n",
      "                current_mean_deviation_rate_in_epoch: 1.530478\n",
      "[12500/20794]\n",
      "                loss: 0.941666\n",
      "                current_mean_loss_in_epoch: 1.176190\n",
      "                current_mean_deviation_rate_in_epoch: 1.532284\n",
      "[13000/20794]\n",
      "                loss: 0.951975\n",
      "                current_mean_loss_in_epoch: 1.177344\n",
      "                current_mean_deviation_rate_in_epoch: 1.536235\n",
      "[13500/20794]\n",
      "                loss: 1.204845\n",
      "                current_mean_loss_in_epoch: 1.179663\n",
      "                current_mean_deviation_rate_in_epoch: 1.548386\n",
      "[14000/20794]\n",
      "                loss: 0.980501\n",
      "                current_mean_loss_in_epoch: 1.179728\n",
      "                current_mean_deviation_rate_in_epoch: 1.550321\n",
      "[14500/20794]\n",
      "                loss: 1.608755\n",
      "                current_mean_loss_in_epoch: 1.181197\n",
      "                current_mean_deviation_rate_in_epoch: 1.547945\n",
      "[15000/20794]\n",
      "                loss: 0.798379\n",
      "                current_mean_loss_in_epoch: 1.180443\n",
      "                current_mean_deviation_rate_in_epoch: 1.543465\n",
      "[15500/20794]\n",
      "                loss: 1.124439\n",
      "                current_mean_loss_in_epoch: 1.180802\n",
      "                current_mean_deviation_rate_in_epoch: 1.542328\n",
      "[16000/20794]\n",
      "                loss: 0.759561\n",
      "                current_mean_loss_in_epoch: 1.179559\n",
      "                current_mean_deviation_rate_in_epoch: 1.542017\n",
      "[16500/20794]\n",
      "                loss: 1.575125\n",
      "                current_mean_loss_in_epoch: 1.179082\n",
      "                current_mean_deviation_rate_in_epoch: 1.536051\n",
      "[17000/20794]\n",
      "                loss: 0.623597\n",
      "                current_mean_loss_in_epoch: 1.178363\n",
      "                current_mean_deviation_rate_in_epoch: 1.531198\n",
      "[17500/20794]\n",
      "                loss: 0.491591\n",
      "                current_mean_loss_in_epoch: 1.178721\n",
      "                current_mean_deviation_rate_in_epoch: 1.531143\n",
      "[18000/20794]\n",
      "                loss: 1.158592\n",
      "                current_mean_loss_in_epoch: 1.178963\n",
      "                current_mean_deviation_rate_in_epoch: 1.524193\n",
      "[18500/20794]\n",
      "                loss: 1.318067\n",
      "                current_mean_loss_in_epoch: 1.179416\n",
      "                current_mean_deviation_rate_in_epoch: 1.520493\n",
      "[19000/20794]\n",
      "                loss: 1.422651\n",
      "                current_mean_loss_in_epoch: 1.181680\n",
      "                current_mean_deviation_rate_in_epoch: 1.523788\n",
      "[19500/20794]\n",
      "                loss: 1.237693\n",
      "                current_mean_loss_in_epoch: 1.182804\n",
      "                current_mean_deviation_rate_in_epoch: 1.523206\n",
      "[20000/20794]\n",
      "                loss: 1.113746\n",
      "                current_mean_loss_in_epoch: 1.180754\n",
      "                current_mean_deviation_rate_in_epoch: 1.523546\n",
      "[20500/20794]\n",
      "                loss: 1.262259\n",
      "                current_mean_loss_in_epoch: 1.180476\n",
      "                current_mean_deviation_rate_in_epoch: 1.519431\n",
      "[    0/ 2599]\n",
      "                    loss: 1.489409\n",
      "                    current_mean_loss_in_epoch: 1.489409\n",
      "                    current_mean_deviation_rate_in_epoch: 2.153941\n",
      "[  500/ 2599]\n",
      "                    loss: 1.345417\n",
      "                    current_mean_loss_in_epoch: 1.145477\n",
      "                    current_mean_deviation_rate_in_epoch: 1.683815\n",
      "[ 1000/ 2599]\n",
      "                    loss: 1.404291\n",
      "                    current_mean_loss_in_epoch: 1.184505\n",
      "                    current_mean_deviation_rate_in_epoch: 1.680033\n",
      "[ 1500/ 2599]\n",
      "                    loss: 0.524013\n",
      "                    current_mean_loss_in_epoch: 1.207062\n",
      "                    current_mean_deviation_rate_in_epoch: 1.628449\n",
      "[ 2000/ 2599]\n",
      "                    loss: 1.261588\n",
      "                    current_mean_loss_in_epoch: 1.212583\n",
      "                    current_mean_deviation_rate_in_epoch: 1.687771\n",
      "[ 2500/ 2599]\n",
      "                    loss: 0.939499\n",
      "                    current_mean_loss_in_epoch: 1.205643\n",
      "                    current_mean_deviation_rate_in_epoch: 1.646907\n",
      "Epoch 2\n",
      "---------------------------------------\n",
      "[    0/20794]\n",
      "                loss: 0.663840\n",
      "                current_mean_loss_in_epoch: 0.663840\n",
      "                current_mean_deviation_rate_in_epoch: 0.604545\n",
      "[  500/20794]\n",
      "                loss: 0.967833\n",
      "                current_mean_loss_in_epoch: 1.191026\n",
      "                current_mean_deviation_rate_in_epoch: 1.494430\n",
      "[ 1000/20794]\n",
      "                loss: 0.723619\n",
      "                current_mean_loss_in_epoch: 1.181524\n",
      "                current_mean_deviation_rate_in_epoch: 1.435358\n",
      "[ 1500/20794]\n",
      "                loss: 1.694278\n",
      "                current_mean_loss_in_epoch: 1.203446\n",
      "                current_mean_deviation_rate_in_epoch: 1.519930\n",
      "[ 2000/20794]\n",
      "                loss: 0.663040\n",
      "                current_mean_loss_in_epoch: 1.194725\n",
      "                current_mean_deviation_rate_in_epoch: 1.525653\n",
      "[ 2500/20794]\n",
      "                loss: 2.032295\n",
      "                current_mean_loss_in_epoch: 1.183056\n",
      "                current_mean_deviation_rate_in_epoch: 1.522461\n",
      "[ 3000/20794]\n",
      "                loss: 0.983926\n",
      "                current_mean_loss_in_epoch: 1.182747\n",
      "                current_mean_deviation_rate_in_epoch: 1.543944\n",
      "[ 3500/20794]\n",
      "                loss: 0.799744\n",
      "                current_mean_loss_in_epoch: 1.179053\n",
      "                current_mean_deviation_rate_in_epoch: 1.517856\n",
      "[ 4000/20794]\n",
      "                loss: 1.931284\n",
      "                current_mean_loss_in_epoch: 1.176321\n",
      "                current_mean_deviation_rate_in_epoch: 1.514654\n",
      "[ 4500/20794]\n",
      "                loss: 1.051550\n",
      "                current_mean_loss_in_epoch: 1.168222\n",
      "                current_mean_deviation_rate_in_epoch: 1.513426\n",
      "[ 5000/20794]\n",
      "                loss: 0.907109\n",
      "                current_mean_loss_in_epoch: 1.175520\n",
      "                current_mean_deviation_rate_in_epoch: 1.508219\n",
      "[ 5500/20794]\n",
      "                loss: 1.054908\n",
      "                current_mean_loss_in_epoch: 1.171023\n",
      "                current_mean_deviation_rate_in_epoch: 1.496951\n",
      "[ 6000/20794]\n",
      "                loss: 0.536634\n",
      "                current_mean_loss_in_epoch: 1.171614\n",
      "                current_mean_deviation_rate_in_epoch: 1.490840\n",
      "[ 6500/20794]\n",
      "                loss: 0.338025\n",
      "                current_mean_loss_in_epoch: 1.170014\n",
      "                current_mean_deviation_rate_in_epoch: 1.484727\n",
      "[ 7000/20794]\n",
      "                loss: 1.118820\n",
      "                current_mean_loss_in_epoch: 1.167734\n",
      "                current_mean_deviation_rate_in_epoch: 1.478764\n",
      "[ 7500/20794]\n",
      "                loss: 1.380643\n",
      "                current_mean_loss_in_epoch: 1.167320\n",
      "                current_mean_deviation_rate_in_epoch: 1.484018\n",
      "[ 8000/20794]\n",
      "                loss: 1.173550\n",
      "                current_mean_loss_in_epoch: 1.169314\n",
      "                current_mean_deviation_rate_in_epoch: 1.487592\n",
      "[ 8500/20794]\n",
      "                loss: 1.371077\n",
      "                current_mean_loss_in_epoch: 1.173329\n",
      "                current_mean_deviation_rate_in_epoch: 1.531931\n",
      "[ 9000/20794]\n",
      "                loss: 2.845296\n",
      "                current_mean_loss_in_epoch: 1.175466\n",
      "                current_mean_deviation_rate_in_epoch: 1.525328\n",
      "[ 9500/20794]\n",
      "                loss: 1.140945\n",
      "                current_mean_loss_in_epoch: 1.169849\n",
      "                current_mean_deviation_rate_in_epoch: 1.513718\n",
      "[10000/20794]\n",
      "                loss: 1.212528\n",
      "                current_mean_loss_in_epoch: 1.169405\n",
      "                current_mean_deviation_rate_in_epoch: 1.520458\n",
      "[10500/20794]\n",
      "                loss: 0.520840\n",
      "                current_mean_loss_in_epoch: 1.167213\n",
      "                current_mean_deviation_rate_in_epoch: 1.516687\n",
      "[11000/20794]\n",
      "                loss: 1.887497\n",
      "                current_mean_loss_in_epoch: 1.170302\n",
      "                current_mean_deviation_rate_in_epoch: 1.524089\n",
      "[11500/20794]\n",
      "                loss: 1.936689\n",
      "                current_mean_loss_in_epoch: 1.173321\n",
      "                current_mean_deviation_rate_in_epoch: 1.518590\n",
      "[12000/20794]\n",
      "                loss: 1.086701\n",
      "                current_mean_loss_in_epoch: 1.174408\n",
      "                current_mean_deviation_rate_in_epoch: 1.526708\n",
      "[12500/20794]\n",
      "                loss: 0.941819\n",
      "                current_mean_loss_in_epoch: 1.175395\n",
      "                current_mean_deviation_rate_in_epoch: 1.528234\n",
      "[13000/20794]\n",
      "                loss: 0.944286\n",
      "                current_mean_loss_in_epoch: 1.176510\n",
      "                current_mean_deviation_rate_in_epoch: 1.531895\n",
      "[13500/20794]\n",
      "                loss: 1.182416\n",
      "                current_mean_loss_in_epoch: 1.178767\n",
      "                current_mean_deviation_rate_in_epoch: 1.542449\n",
      "[14000/20794]\n",
      "                loss: 0.984812\n",
      "                current_mean_loss_in_epoch: 1.178787\n",
      "                current_mean_deviation_rate_in_epoch: 1.544225\n",
      "[14500/20794]\n",
      "                loss: 1.595385\n",
      "                current_mean_loss_in_epoch: 1.180241\n",
      "                current_mean_deviation_rate_in_epoch: 1.542006\n",
      "[15000/20794]\n",
      "                loss: 0.783501\n",
      "                current_mean_loss_in_epoch: 1.179404\n",
      "                current_mean_deviation_rate_in_epoch: 1.537586\n",
      "[15500/20794]\n",
      "                loss: 1.104881\n",
      "                current_mean_loss_in_epoch: 1.179848\n",
      "                current_mean_deviation_rate_in_epoch: 1.537001\n",
      "[16000/20794]\n",
      "                loss: 0.757474\n",
      "                current_mean_loss_in_epoch: 1.178615\n",
      "                current_mean_deviation_rate_in_epoch: 1.536437\n",
      "[16500/20794]\n",
      "                loss: 1.549509\n",
      "                current_mean_loss_in_epoch: 1.178043\n",
      "                current_mean_deviation_rate_in_epoch: 1.530265\n",
      "[17000/20794]\n",
      "                loss: 0.626356\n",
      "                current_mean_loss_in_epoch: 1.177391\n",
      "                current_mean_deviation_rate_in_epoch: 1.525868\n",
      "[17500/20794]\n",
      "                loss: 0.501460\n",
      "                current_mean_loss_in_epoch: 1.177745\n",
      "                current_mean_deviation_rate_in_epoch: 1.525873\n",
      "[18000/20794]\n",
      "                loss: 1.157904\n",
      "                current_mean_loss_in_epoch: 1.177995\n",
      "                current_mean_deviation_rate_in_epoch: 1.519006\n",
      "[18500/20794]\n",
      "                loss: 1.283923\n",
      "                current_mean_loss_in_epoch: 1.178359\n",
      "                current_mean_deviation_rate_in_epoch: 1.515119\n",
      "[19000/20794]\n",
      "                loss: 1.420665\n",
      "                current_mean_loss_in_epoch: 1.180644\n",
      "                current_mean_deviation_rate_in_epoch: 1.518435\n",
      "[19500/20794]\n",
      "                loss: 1.264711\n",
      "                current_mean_loss_in_epoch: 1.181815\n",
      "                current_mean_deviation_rate_in_epoch: 1.517754\n",
      "[20000/20794]\n",
      "                loss: 1.119511\n",
      "                current_mean_loss_in_epoch: 1.179739\n",
      "                current_mean_deviation_rate_in_epoch: 1.518182\n",
      "[20500/20794]\n",
      "                loss: 1.218487\n",
      "                current_mean_loss_in_epoch: 1.179560\n",
      "                current_mean_deviation_rate_in_epoch: 1.514454\n",
      "[    0/ 2599]\n",
      "                    loss: 1.495310\n",
      "                    current_mean_loss_in_epoch: 1.495310\n",
      "                    current_mean_deviation_rate_in_epoch: 2.140487\n",
      "[  500/ 2599]\n",
      "                    loss: 1.352275\n",
      "                    current_mean_loss_in_epoch: 1.149482\n",
      "                    current_mean_deviation_rate_in_epoch: 1.685536\n",
      "[ 1000/ 2599]\n",
      "                    loss: 1.406642\n",
      "                    current_mean_loss_in_epoch: 1.187990\n",
      "                    current_mean_deviation_rate_in_epoch: 1.689181\n",
      "[ 1500/ 2599]\n",
      "                    loss: 0.524213\n",
      "                    current_mean_loss_in_epoch: 1.210404\n",
      "                    current_mean_deviation_rate_in_epoch: 1.645707\n",
      "[ 2000/ 2599]\n",
      "                    loss: 1.259539\n",
      "                    current_mean_loss_in_epoch: 1.215874\n",
      "                    current_mean_deviation_rate_in_epoch: 1.697895\n",
      "[ 2500/ 2599]\n",
      "                    loss: 0.969892\n",
      "                    current_mean_loss_in_epoch: 1.208400\n",
      "                    current_mean_deviation_rate_in_epoch: 1.654963\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "writer = SummaryWriter()\n",
    "datetime_str = str(datetime.datetime.now()).replace(\" \", \"_\")\n",
    "os.makedirs(\n",
    "    name=f\"checkpoints/{datetime_str}\",\n",
    "    exist_ok=True,\n",
    ")\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\\n---------------------------------------\")\n",
    "    train(\n",
    "        dataloader=train_dl,\n",
    "        model=landnet,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        current_epoch=epoch,\n",
    "        writer=writer,\n",
    "    )\n",
    "    summary_csv_path = os.path.join(\"checkpoints\", datetime_str, f\"landnet_val_epoch{epoch}.csv\")\n",
    "    validate(\n",
    "        dataloader=val_dl,\n",
    "        model=landnet,\n",
    "        loss_func=loss_func,\n",
    "        current_epoch=epoch,\n",
    "        writer=writer,\n",
    "        summary_csv_path=summary_csv_path\n",
    "    )\n",
    "    torch.save(\n",
    "        obj=landnet,\n",
    "        f=os.path.join(\"checkpoints\", datetime_str, f\"landnet_checkpoint_epoch{epoch}.pth\")\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0/ 2600]\n",
      "                    loss: 1.330128\n",
      "                    current_mean_loss_in_epoch: 1.330128\n",
      "                    current_mean_deviation_rate_in_epoch: 2.162755\n",
      "[  500/ 2600]\n",
      "                    loss: 1.001151\n",
      "                    current_mean_loss_in_epoch: 1.186224\n",
      "                    current_mean_deviation_rate_in_epoch: 1.506972\n",
      "[ 1000/ 2600]\n",
      "                    loss: 1.237408\n",
      "                    current_mean_loss_in_epoch: 1.164990\n",
      "                    current_mean_deviation_rate_in_epoch: 1.599910\n",
      "[ 1500/ 2600]\n",
      "                    loss: 1.099955\n",
      "                    current_mean_loss_in_epoch: 1.165299\n",
      "                    current_mean_deviation_rate_in_epoch: 1.583691\n",
      "[ 2000/ 2600]\n",
      "                    loss: 1.751282\n",
      "                    current_mean_loss_in_epoch: 1.186366\n",
      "                    current_mean_deviation_rate_in_epoch: 1.708447\n",
      "[ 2500/ 2600]\n",
      "                    loss: 0.755632\n",
      "                    current_mean_loss_in_epoch: 1.181996\n",
      "                    current_mean_deviation_rate_in_epoch: 1.642395\n"
     ]
    }
   ],
   "source": [
    "validate(\n",
    "    dataloader=test_dl,\n",
    "    model=landnet,\n",
    "    loss_func=loss_func,\n",
    "    current_epoch=0,\n",
    "    summary_csv_path=os.path.join(\"checkpoints\", datetime_str, \"landnet_test.csv\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
